# -*- coding: utf-8 -*-
"""neural network test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yukCJ7jGX0R38_iJ7ifzwC96U3cvtbWB
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

data = pd.read_csv('digit_data.csv')
data

data.head()

data.info()

data = np.array(data)
m, n = data.shape
np.random.shuffle(data) # shuffle before splitting into dev and training sets

data_test = data[0:1000].T
Y_test = data_test[0]
X_test = data_test[1:n]
X_test = X_test / 255.

data_train = data[1000:m].T
Y_train = data_train[0]
X_train = data_train[1:n]
X_train = X_train / 255.
_,m_train = X_train.shape

Y_train

# Generate a random index in the range dedicated for training images
random_index = np.random.randint(0, m_train)

# Plot the image corresponding to the generated index
def plot_image_with_label(index, X, Y):
    current_image = X[:, index].reshape((28, 28)) * 255
    label = Y[index]
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.title(f" Label: {label}")
    plt.show()

plot_image_with_label(random_index, X_train, Y_train)

def init_params():
    W1 = np.random.rand(10, 784) - 0.5
    b1 = np.random.rand(10, 1) - 0.5
    W2 = np.random.rand(10, 10) - 0.5
    b2 = np.random.rand(10, 1) - 0.5
    return W1, b1, W2, b2

def ReLU(Z):
    return np.maximum(Z, 0)

def plot_activation_function(input_range, activation_function, title):
    plt.plot(input_range, activation_function(input_range))
    plt.title(title)
    plt.xlabel('Input')
    plt.ylabel('Output')
    plt.grid()
    plt.show()

# Generate a range of values for the input
x_range = np.linspace(-10, 10, 200)

# Plot the ReLU activation function
plot_activation_function(x_range, ReLU, 'ReLU Activation Function')

def ReLU_deriv(Z):
    return Z > 0

# Plot the derivative of ReLU activation function
plot_activation_function(x_range, ReLU_deriv, 'Derivative of ReLU Activation Function')

def softmax(Z):
    A = np.exp(Z) / sum(np.exp(Z))
    return A

# Plot the softmax activation function
plot_activation_function(x_range, softmax, 'Softmax Activation Function')

def forward_prop(W1, b1, W2, b2, X):
    Z1 = W1.dot(X) + b1
    A1 = ReLU(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

def one_hot(Y):
    Y = Y.astype(int)  # Convert Y to integer type
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    return one_hot_Y

# def one_hot(Y):
#     one_hot_Y = np.zeros((Y.size, Y.max() + 1))
#     one_hot_Y[np.arange(Y.size), Y] = 1
#     one_hot_Y = one_hot_Y.T
#     return one_hot_Y

# Example ground truth vector
Y = np.array([0, 2, 1, 0, 3])

# Apply one-hot encoding
one_hot_matrix = one_hot(Y)
print(one_hot_matrix)

def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):
    one_hot_Y = one_hot(Y)
    dZ2 = A2 - one_hot_Y
    dW2 = 1 / m * dZ2.dot(A1.T)
    db2 = 1 / m * np.sum(dZ2)
    # db2 =  1 / m * np.sum(dZ2, axis=1).reshape(-1, 1)
    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)
    dW1 = 1 / m * dZ1.dot(X.T)
    db1 = 1 / m * np.sum(dZ1)
    # db1 = 1 / m * np.sum(dZ1, axis=1).reshape(-1, 1)
    return dW1, db1, dW2, db2

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
    W1 = W1 - alpha * dW1
    b1 = b1 - alpha * db1
    W2 = W2 - alpha * dW2
    b2 = b2 - alpha * db2
    return W1, b1, W2, b2

def get_predictions(A2):
    return np.argmax(A2, 0)

def get_accuracy(predictions, Y):
    print(predictions, Y)
    return np.sum(predictions == Y) / Y.size

# Example predicted vector and ground truth vector
predicted = np.array([0, 1, 1, 0, 2])
ground_truth = np.array([0, 1, 1, 0, 3])

# Calculate accuracy
accuracy = get_accuracy(predicted, ground_truth)
print("Accuracy:", accuracy)

def gradient_descent(X, Y, alpha, iterations):
    W1, b1, W2, b2 = init_params()
    accuracy_scores = []
    for i in range(iterations):
        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)
        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)
        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)
        if i % 5 == 0:
           predictions = get_predictions(A2)
           accuracy = get_accuracy(predictions, Y)
           print("Iteration:", i, "Accuracy:", accuracy)
           accuracy_scores.append((i, accuracy))
    return W1, b1, W2, b2, accuracy_scores
    #         print("Iteration: ", i)
    #         predictions = get_predictions(A2)
    #         print(get_accuracy(predictions, Y))
    # return W1, b1, W2, b2

W1, b1, W2, b2, accuracy_scores = gradient_descent(X_train, Y_train, 0.10, 500)

# Extract iteration numbers and accuracy scores from the list
iterations, accuracies = zip(*accuracy_scores)

# Plot accuracy vs. iterations
plt.plot(iterations, accuracies)
plt.title('Accuracy vs. Iterations')
plt.xlabel('Iterations')
plt.ylabel('Accuracy')
plt.grid()
plt.show()

def make_predictions(X, W1, b1, W2, b2):
    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)
    predictions = get_predictions(A2)
    return predictions

def test_prediction(index, W1, b1, W2, b2):
    current_image = X_train[:, index, None]
    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)
    label = Y_train[index]
    print("Prediction: ", prediction)
    print("Label: ", label)

    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

test_prediction(0, W1, b1, W2, b2)
test_prediction(8, W1, b1, W2, b2)
test_prediction(5, W1, b1, W2, b2)
test_prediction(7, W1, b1, W2, b2)

test_prediction(168, W1, b1, W2, b2)

test_predictions = make_predictions(X_test, W1, b1, W2, b2)
get_accuracy(test_predictions, Y_test)

"""# Multiple hidden layer

"""

def init_params():
    W1 = np.random.rand(10, 784) - 0.5
    b1 = np.random.rand(10, 1) - 0.5


    W2 = np.random.rand(15, 10) - 0.5
    b2 = np.random.rand(15, 1) - 0.5

    W3 = np.random.rand(12,15) - 0.5
    b3 = np.random.rand(12, 1) - 0.5

    W4 = np.random.rand(10,12) - 0.5
    b4 = np.random.rand(10, 1) - 0.5

    return W1, b1, W2, b2, W3, b3, W4, b4

def sigmoid(x):

    return 1 / (1 + np.exp(-x))

plot_activation_function(sigmoid, 'sigmoid function', x_range)

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

plot_activation_function(sigmoid_derivative, 'sigmoid function', x_range)

def tanh(x):

    return np.tanh(x)

def plot_activation_function(func, func_name, x_range):
    y = func(x_range)
    plt.plot(x_range, y, label=func_name)
    plt.title(f'{func_name} Activation Function')
    plt.xlabel('Input')
    plt.ylabel('Output')
    plt.legend()
    plt.grid()
    plt.show()

# Generate a range of values for the input
x_range = np.linspace(-10, 10, 200)

# Plot the tanh activation function using the plot_activation_function function
plot_activation_function(tanh, 'Hyperbolic Tangent (tanh)', x_range)

def tanh_derivative(x):
    return 1 - tanh(x)**2

plot_activation_function(tanh_derivative, 'Hyperbolic Tangent (tanh)', x_range)

def forward_prop(W1, b1, W2, b2,W3,b3,W4,b4, X):
    Z1 = W1.dot(X) + b1
    A1 = ReLU(Z1)

    Z2 = W2.dot(A1) + b2
    A2 = sigmoid(Z2)

    Z3 = W3.dot(A2) + b3
    A3 = tanh(Z3)

    Z4 = W4.dot(A3) + b4
    A4 = softmax(Z4)
    return Z1, A1, Z2, A2, Z3,A3,Z4,A4

def one_hot(Y):
    Y = Y.astype(int)  # Convert Y to integer type
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    return one_hot_Y

def backward_prop(Z1, A1, Z2, A2, Z3, A3, Z4,A4, W1, W2,W3,W4, X, Y):
    one_hot_Y = one_hot(Y)

    dZ4 = A4 - one_hot_Y
    dW4 = 1 / m * dZ4.dot(A3.T)
    db4 = 1 / m * np.sum(dZ4)

    dZ3 = W4.T.dot(dZ4) * tanh_derivative(Z3)
    dW3 = 1 / m * dZ3.dot(A2.T)
    db3 = 1 / m * np.sum(dZ3)

    dZ2 = W3.T.dot(dZ3) * sigmoid_derivative(Z2)
    dW2 = 1 / m * dZ2.dot(A1.T)
    db2 = 1 / m * np.sum(dZ2)

    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)
    dW1 = 1 / m * dZ1.dot(X.T)
    db1 = 1 / m * np.sum(dZ1)




    return dW1, db1, dW2, db2, dW3, db3, dW4,db4

def update_params(W1, b1, W2, b2,W3,b3,W4,b4, dW1, db1, dW2, db2,dW3,db3,dW4,db4, alpha):
    W1 = W1 - alpha * dW1
    b1 = b1 - alpha * db1

    W2 = W2 - alpha * dW2
    b2 = b2 - alpha * db2

    W3 = W3 - alpha * dW3
    b3 = b3 - alpha * db3

    W4 = W4 - alpha * dW4
    b4 = b4 - alpha * db4

    return W1, b1, W2, b2, W3, b3, W4, b4

def get_predictions(A2):
    return np.argmax(A2, 0)

def get_accuracy(predictions, Y):
    print(predictions, Y)
    return np.sum(predictions == Y) / Y.size
# Example predicted vector and ground truth vector
predicted = np.array([0, 1, 1, 0, 2])
ground_truth = np.array([0, 1, 1, 0, 3])
# Calculate accuracy
accuracy = get_accuracy(predicted, ground_truth)
print("Accuracy:", accuracy)

def gradient_descent(X, Y, alpha, iterations):
    W1, b1, W2, b2, W3, b3, W4, b4 = init_params()
    accuracy_scores = []
    for i in range(iterations):
        Z1, A1, Z2, A2, Z3, A3, Z4, A4 = forward_prop(W1, b1, W2, b2,W3,b3,W4,b4, X)
        dW1, db1, dW2, db2, dW3, db3, dW4,db4 = backward_prop(Z1, A1, Z2, A2, Z3, A3, Z4,A4, W1, W2,W3,W4, X, Y)
        W1, b1, W2, b2, W3, b3, W4, b4 = update_params(W1, b1, W2, b2,W3,b3,W4,b4, dW1, db1, dW2, db2,dW3,db3,dW4,db4, alpha)
        if i % 10 == 0:
            print("Iteration: ", i)
            predictions = get_predictions(A4)
            accuracy = get_accuracy(predictions, Y)
            print(accuracy)
            accuracy_scores.append((i, accuracy))

    # Convert accuracy_scores to separate lists for plotting
    iterations, accuracies = zip(*accuracy_scores)

    # Plotting accuracy vs iterations
    plt.plot(iterations, accuracies)
    plt.xlabel('Iterations')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Iterations')
    plt.show()

    return W1, b1, W2, b2, W3,b3,W4,b4

W1, b1, W2, b2, W3,b3,W4,b4 = gradient_descent(X_train, Y_train, 0.10, 500)

def make_predictions(X, W1, b1, W2, b2,W3,b3,W4,b4):
    Z1, A1, Z2, A2, Z3,A3,Z4,A4 = forward_prop(W1, b1, W2, b2,W3,b3,W4,b4, X)
    predictions = get_predictions(A4)
    return predictions

def test_prediction(index, W1, b1, W2, b2,W3,b3,W4,b4):
    current_image = X_train[:, index, None]
    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2,W3,b3,W4,b4)
    label = Y_train[index]
    print("Prediction: ", prediction)
    print("Label: ", label)

    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

test_prediction(0, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(1, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(2, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(3, W1, b1, W2, b2,W3,b3,W4,b4)

test_predictions = make_predictions(X_test, W1, b1, W2, b2, W3, b3, W4, b4)
get_accuracy(test_predictions, Y_test)

"""# Different initialization

"""

def init_params_kaiming():
    def kaiming_initializer(n_in):
        return np.random.normal(0, np.sqrt(2 / n_in))

    W1 = kaiming_initializer(784) * np.random.randn(10, 784)
    b1 = np.zeros((10, 1))

    W2 = kaiming_initializer(10) * np.random.randn(15, 10)
    b2 = np.zeros((15, 1))

    W3 = kaiming_initializer(15) * np.random.randn(12, 15)
    b3 = np.zeros((12, 1))

    W4 = kaiming_initializer(12) * np.random.randn(10, 12)
    b4 = np.zeros((10, 1))

    return W1, b1, W2, b2, W3, b3, W4, b4

def init_params_xavier():
    def xavier_initializer(n_in, n_out):
        bound = np.sqrt(6 / (n_in + n_out))
        return np.random.uniform(-bound, bound)

    W1 = xavier_initializer(784, 10) * np.random.rand(10, 784)
    b1 = np.random.rand(10, 1) - 0.5

    W2 = xavier_initializer(10, 15) * np.random.rand(15, 10)
    b2 = np.random.rand(15, 1) - 0.5

    W3 = xavier_initializer(15, 12) * np.random.rand(12, 15)
    b3 = np.random.rand(12, 1) - 0.5

    W4 = xavier_initializer(12, 10) * np.random.rand(10, 12)
    b4 = np.random.rand(10, 1) - 0.5

    return W1, b1, W2, b2, W3, b3, W4, b4

def gradient_descent(X, Y, alpha, iterations):
    W1, b1, W2, b2, W3, b3, W4, b4 = init_params_xavier()
    accuracy_scores = []  # To store accuracy scores at different iterations
    for i in range(iterations):
        Z1, A1, Z2, A2, Z3, A3, Z4, A4 = forward_prop(W1, b1, W2, b2,W3,b3,W4,b4, X)
        dW1, db1, dW2, db2, dW3, db3, dW4,db4 = backward_prop(Z1, A1, Z2, A2, Z3, A3, Z4,A4, W1, W2,W3,W4, X, Y)
        W1, b1, W2, b2, W3, b3, W4, b4 = update_params(W1, b1, W2, b2,W3,b3,W4,b4, dW1, db1, dW2, db2,dW3,db3,dW4,db4, alpha)
        if i % 10 == 0:
            print("Iteration: ", i)
            predictions = get_predictions(A4)
            accuracy = get_accuracy(predictions, Y)
            print(accuracy)
            accuracy_scores.append((i, accuracy))
    # Convert accuracy_scores to separate lists for plotting
    iterations, accuracies = zip(*accuracy_scores)

    # Plotting accuracy vs iterations
    plt.plot(iterations, accuracies)
    plt.xlabel('Iterations')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Iterations')
    plt.show()

    return W1, b1, W2, b2, W3,b3,W4,b4

W1, b1, W2, b2, W3,b3,W4,b4 = gradient_descent(X_train, Y_train, 0.10, 500)

def make_predictions(X, W1, b1, W2, b2,W3,b3,W4,b4):
    Z1, A1, Z2, A2, Z3,A3,Z4,A4 = forward_prop(W1, b1, W2, b2,W3,b3,W4,b4, X)
    predictions = get_predictions(A4)
    return predictions

def test_prediction(index, W1, b1, W2, b2,W3,b3,W4,b4):
    current_image = X_train[:, index, None]
    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2,W3,b3,W4,b4)
    label = Y_train[index]
    print("Prediction: ", prediction)
    print("Label: ", label)

    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

test_prediction(0, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(1, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(2, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(3, W1, b1, W2, b2,W3,b3,W4,b4)

test_predictions = make_predictions(X_test, W1, b1, W2, b2, W3, b3, W4, b4)
get_accuracy(test_predictions, Y_test)

"""# Kaiming Initialization"""

def init_params_kaiming():
    def kaiming_initializer(n_in):
        return np.random.normal(0, np.sqrt(2 / n_in))

    W1 = kaiming_initializer(784) * np.random.randn(10, 784)
    b1 = np.zeros((10, 1))

    W2 = kaiming_initializer(10) * np.random.randn(15, 10)
    b2 = np.zeros((15, 1))

    W3 = kaiming_initializer(15) * np.random.randn(12, 15)
    b3 = np.zeros((12, 1))

    W4 = kaiming_initializer(12) * np.random.randn(10, 12)
    b4 = np.zeros((10, 1))

    return W1, b1, W2, b2, W3, b3, W4, b4

def gradient_descent(X, Y, alpha, iterations):
    W1, b1, W2, b2, W3, b3, W4, b4 = init_params_kaiming()
    accuracy_scores = []
    for i in range(iterations):
        Z1, A1, Z2, A2, Z3, A3, Z4, A4 = forward_prop(W1, b1, W2, b2,W3,b3,W4,b4, X)
        dW1, db1, dW2, db2, dW3, db3, dW4,db4 = backward_prop(Z1, A1, Z2, A2, Z3, A3, Z4,A4, W1, W2,W3,W4, X, Y)
        W1, b1, W2, b2, W3, b3, W4, b4 = update_params(W1, b1, W2, b2,W3,b3,W4,b4, dW1, db1, dW2, db2,dW3,db3,dW4,db4, alpha)
        if i % 10 == 0:
            print("Iteration: ", i)
            predictions = get_predictions(A4)
            accuracy = get_accuracy(predictions, Y)
            print(accuracy)
            accuracy_scores.append((i, accuracy))

    # Convert accuracy_scores to separate lists for plotting
    iterations, accuracies = zip(*accuracy_scores)

    # Plotting accuracy vs iterations
    plt.plot(iterations, accuracies)
    plt.xlabel('Iterations')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Iterations')
    plt.show()

    return W1, b1, W2, b2, W3,b3,W4,b4

W1, b1, W2, b2, W3,b3,W4,b4 = gradient_descent(X_train, Y_train, 0.10, 500)

test_prediction(0, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(1, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(2, W1, b1, W2, b2,W3,b3,W4,b4)
test_prediction(3, W1, b1, W2, b2,W3,b3,W4,b4)

test_predictions = make_predictions(X_test, W1, b1, W2, b2, W3, b3, W4, b4)
get_accuracy(test_predictions, Y_test)

"""# Dropout layer"""

def init_params():
    W1 = np.random.rand(10, 784) - 0.5
    b1 = np.random.rand(10, 1) - 0.5
    W2 = np.random.rand(10, 10) - 0.5
    b2 = np.random.rand(10, 1) - 0.5
    return W1, b1, W2, b2

def ReLU(Z):
    return np.maximum(Z, 0)

def ReLU_deriv(Z):
    return Z > 0

def softmax(Z):
    A = np.exp(Z) / sum(np.exp(Z))
    return A

def forward_prop(W1, b1, W2, b2, X):
    Z1 = W1.dot(X) + b1
    A1 = ReLU(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

def one_hot(Y):
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    return one_hot_Y

def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):
    one_hot_Y = one_hot(Y)
    dZ2 = A2 - one_hot_Y
    dW2 = 1 / m * dZ2.dot(A1.T)
    db2 = 1 / m * np.sum(dZ2)
    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)
    dW1 = 1 / m * dZ1.dot(X.T)
    db1 = 1 / m * np.sum(dZ1)
    return dW1, db1, dW2, db2

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
    W1 = W1 - alpha * dW1
    b1 = b1 - alpha * db1
    W2 = W2 - alpha * dW2
    b2 = b2 - alpha * db2
    return W1, b1, W2, b2

def get_predictions(A2):
    return np.argmax(A2, 0)

def get_accuracy(predictions, Y):
    print(predictions, Y)
    return np.sum(predictions == Y) / Y.size

# Example predicted vector and ground truth vector
predicted = np.array([0, 1, 1, 0, 2])
ground_truth = np.array([0, 1, 1, 0, 3])

# Calculate accuracy
accuracy = get_accuracy(predicted, ground_truth)
print("Accuracy:", accuracy)

def dropout_forward(X, keep_prob):
    D = np.random.rand(*X.shape) < keep_prob
    A = X * D / keep_prob
    return A, D

def visualize_dropout(original_data, dropout_data, dropout_mask):
    fig, axs = plt.subplots(1, 3, figsize=(12, 4))

    axs[0].imshow(original_data.reshape(28, 28), cmap='gray')
    axs[0].set_title("Original Data")

    axs[1].imshow(dropout_mask.reshape(28, 28), cmap='gray')
    axs[1].set_title("Dropout Mask")

    axs[2].imshow(dropout_data.reshape(28, 28), cmap='gray')
    axs[2].set_title("Data with Dropout")

    for ax in axs:
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# # Example usage
# input_size, m = X_train.shape # Number of examples
# X = X_train.copy()
# keep_prob = 0.8

# # Select a random example index for visualization
# example_index = np.random.randint(0, m)
# print(example_index)
# example_data = X[:, example_index].reshape(input_size, 1)

# A, D = dropout_forward(example_data, keep_prob)
# visualize_dropout(example_data, A, D)

def gradient_descent(X, Y, alpha, iterations, keep_prob):
    W1, b1, W2, b2 = init_params()
    accuracy_scores = []
    for i in range(iterations):
        A, D = dropout_forward(X, keep_prob)
        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, A)
        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, A, Y)
        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)
        if i % 10 == 0:
            print("Iteration: ", i)
            predictions = get_predictions(A2)
            accuracy = get_accuracy(predictions, Y)
            print(accuracy)
            accuracy_scores.append((i, accuracy))

    # Convert accuracy_scores to separate lists for plotting
    iterations, accuracies = zip(*accuracy_scores)

    # Plotting accuracy vs iterations
    plt.plot(iterations, accuracies)
    plt.xlabel('Iterations')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Iterations')
    plt.show()

    return W1, b1, W2, b2

keep_prob = 0.8
W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500, keep_prob)

def make_predictions(X, W1, b1, W2, b2):
    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)
    predictions = get_predictions(A2)
    return predictions

def test_prediction(index, W1, b1, W2, b2):
    current_image = X_train[:, index, None]
    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)
    label = Y_train[index]
    print("Prediction: ", prediction)
    print("Label: ", label)

    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

test_prediction(0, W1, b1, W2, b2)
test_prediction(1, W1, b1, W2, b2)
test_prediction(2, W1, b1, W2, b2)
test_prediction(3, W1, b1, W2, b2)

test_predictions = make_predictions(X_test, W1, b1, W2, b2)
get_accuracy(test_predictions, Y_test)